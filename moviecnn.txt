
from keras.datasets import imdb
((XT,YT),(Xt,Yt)) = imdb.load_data(num_words=10000)

len(XT)
len(Xt)
print(XT[0])

word_idx = imdb.get_word_index()
print(word_idx.items())

idx_word = dict([value,key] for (key,value) in word_idx.items())
print(idx_word.items())
actual_review = ' '.join([idx_word.get(idx-3,'?') for idx in XT[0]])
print(actual_review)
## Next Step - Vectorize the Data
## Vocab Size - 10,000 We will make sure every sentence is represented by a vector
import numpy as np
def vectorize_sentences(sentences,dim=10000):
  
  outputs = np.zeros((len(sentences),dim))
  
  for i,idx in enumerate(sentences):
    outputs[i,idx] = 1
   
  return outputs
X_train = vectorize_sentences(XT)
X_test = vectorize_sentences(Xt)

print(X_train.shape)
print(X_test.shape)
print(X_train[0])

Y_train = np.asarray(YT).astype('float32')
Y_test = np.asarray(Yt).astype('float32')

from keras import models
from keras.layers import Dense
# Define the model
model = models.Sequential()
model.add(Dense(16,activation='relu',input_shape=(10000,)))
model.add(Dense(16,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
# Compile the Model
model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])
model.summary()

x_val = X_train[:5000]
x_train_new = X_train[5000:]

y_val = Y_train[:5000]
y_train_new = Y_train[5000:]
hist = model.fit(x_train_new,y_train_new,epochs=20,batch_size=512,validation_data=(x_val,y_val))
import matplotlib.pyplot as plt

h = hist.history
plt.plot(h['val_loss'],label="Validation Loss")
plt.plot(h['loss'],label="Training Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
plt.plot(h['val_accuracy'],label="Validation Acc")
plt.plot(h['accuracy'],label="Training Acc")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
model.evaluate(X_test,Y_test)[1]
model.evaluate(X_train,Y_train)[1]
model.predict(X_test)